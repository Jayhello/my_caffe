//
// Created by root on 9/19/18.
//

#include "sgd_solvers.h"
#include "device_alternate.h"
#include "io.h"
#include "upgrade_proto.h"
#include "solver_factory.h"


namespace caffe{

    template <typename Dtype>
    Dtype SGDSolver<Dtype>::GetLearningRate() {
        Dtype rate;
        const string& lr_policy = this->param_.lr_policy();
        if (lr_policy == "fixed") {
            rate = this->param_.base_lr();
        } else if (lr_policy == "step") {
            this->current_step_ = this->iter_ / this->param_.stepsize();
            rate = this->param_.base_lr() *
                   pow(this->param_.gamma(), this->current_step_);
        } else if (lr_policy == "exp") {
            rate = this->param_.base_lr() * pow(this->param_.gamma(), this->iter_);
        } else if (lr_policy == "inv") {
            rate = this->param_.base_lr() *
                   pow(Dtype(1) + this->param_.gamma() * this->iter_,
                       - this->param_.power());
        } else if (lr_policy == "multistep") {
            if (this->current_step_ < this->param_.stepvalue_size() &&
                this->iter_ >= this->param_.stepvalue(this->current_step_)) {
                this->current_step_++;
                LOG(INFO) << "MultiStep Status: Iteration " <<
                          this->iter_ << ", step = " << this->current_step_;
            }
            rate = this->param_.base_lr() *
                   pow(this->param_.gamma(), this->current_step_);
        } else if (lr_policy == "poly") {
            rate = this->param_.base_lr() * pow(Dtype(1.) -
                                                (Dtype(this->iter_) / Dtype(this->param_.max_iter())),
                                                this->param_.power());
        } else if (lr_policy == "sigmoid") {
            rate = this->param_.base_lr() * (Dtype(1.) /
                                             (Dtype(1.) + exp(-this->param_.gamma() * (Dtype(this->iter_) -
                                                                                       Dtype(this->param_.stepsize())))));
        } else {
            LOG(FATAL) << "Unknown learning rate policy: " << lr_policy;
        }
        return rate;
    }

    template <typename Dtype>
    void SGDSolver<Dtype>::PreSolve() {
        // Initialize the history
        const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
        history_.clear();
        update_.clear();
        temp_.clear();
        for (int i = 0; i < net_params.size(); ++i) {
            const vector<int>& shape = net_params[i]->shape();
            history_.push_back(shared_ptr<Blob<Dtype> >(new Blob<Dtype>(shape)));
            update_.push_back(shared_ptr<Blob<Dtype> >(new Blob<Dtype>(shape)));
            temp_.push_back(shared_ptr<Blob<Dtype> >(new Blob<Dtype>(shape)));
        }
    }


    template <typename Dtype>
    void SGDSolver<Dtype>::ClipGradients() {
        const Dtype clip_gradients = this->param_.clip_gradients();
        if (clip_gradients < 0) { return; }
        const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
        Dtype sumsq_diff = 0;
        for (int i = 0; i < net_params.size(); ++i) {
            sumsq_diff += net_params[i]->sumsq_diff();
        }
        const Dtype l2norm_diff = std::sqrt(sumsq_diff);
        if (l2norm_diff > clip_gradients) {
            Dtype scale_factor = clip_gradients / l2norm_diff;
            LOG(INFO) << "Gradient clipping: scaling down gradients (L2 norm "
                      << l2norm_diff << " > " << clip_gradients << ") "
                      << "by scale factor " << scale_factor;
            for (int i = 0; i < net_params.size(); ++i) {
                net_params[i]->scale_diff(scale_factor);
            }
        }
    }

    template <typename Dtype>
    void SGDSolver<Dtype>::ApplyUpdate() {
        CHECK(Caffe::root_solver());

        // GetLearningRate根据设置的lr_policy来计算当前迭代的learning rate的值
        Dtype rate = GetLearningRate();

        // 判断是否需要输出当前的learning rate
        if (this->param_.display() && this->iter_ % this->param_.display() == 0) {
            LOG(INFO) << "Iteration " << this->iter_ << ", lr = " << rate;
        }

        // 避免梯度爆炸，如果梯度的二范数超过了某个数值则进行scale操作，将梯度减小
        ClipGradients();

        // 对所有可更新的网络参数进行操作
        for (int param_id = 0; param_id < this->net_->learnable_params().size();
             ++param_id) {
            // 将第param_id个参数的梯度除以iter_size，
            // 这一步的作用是保证实际的batch_size=iter_size*设置的batch_size
            Normalize(param_id);

            // 将正则化部分的梯度降入到每个参数的梯度中
            Regularize(param_id);

            // 计算SGD算法的梯度(momentum等)
            ComputeUpdateValue(param_id, rate);
        }
        // 调用`Net::Update`更新所有的参数
        this->net_->Update();
    }

    template <typename Dtype>
    void SGDSolver<Dtype>::Normalize(int param_id) {
        // 如果iter_size的值为1，则不需要任何处理直接return
        if (this->param_.iter_size() == 1) { return; }

        // Scale gradient to counterbalance accumulation.
        // 通过net_返回所有可以学习的参数，是一个vector<shared_ptr<Blob<Dtype> > >
        const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();

        // 要乘以的系数等于1/iter_size
        const Dtype accum_normalization = Dtype(1.) / this->param_.iter_size();

        switch (Caffe::mode()) {
            case Caffe::CPU: {
                // caffe_scal在/CAFFE_ROOT/src/caffe/util/math_functions.cpp中
                // 是blas的scale函数的一个封装，第一个参数是数据的个数，第二个参数是乘以的系数，
                // 第三个参数是数据的指针
                caffe_scal(net_params[param_id]->count(), accum_normalization,
                           net_params[param_id]->mutable_cpu_diff());
                break;
            }
            case Caffe::GPU: {
#ifndef CPU_ONLY
                caffe_gpu_scal(net_params[param_id]->count(), accum_normalization,
        net_params[param_id]->mutable_gpu_diff());
#else
                NO_GPU;
#endif
                break;
            }
            default:
                LOG(FATAL) << "Unknown caffe mode: " << Caffe::mode();
        }
    }

    template <typename Dtype>
    void SGDSolver<Dtype>::Regularize(int param_id) {
        // 获取所有可以学习的参数的vector
        const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
        // 获取所有的参数对应的weight_decay的vector
        const vector<float>& net_params_weight_decay =
                this->net_->params_weight_decay();
        // 模型整体的weight_decay数值,正则化权重
        Dtype weight_decay = this->param_.weight_decay();

        // 获取正则化的类型：L1 或 L2，默认为L2
        string regularization_type = this->param_.regularization_type();

        // 实际的weight_decay等于整体模型的数值乘以具体每个参数的数值
        Dtype local_decay = weight_decay * net_params_weight_decay[param_id];

        switch (Caffe::mode()) {
            case Caffe::CPU: {
                // 如果weight_decay不为0，则计算
                if (local_decay) {
                    if (regularization_type == "L2") {
                        // add weight decay
                        // L2的梯度为diff_ = weight_decay*data_ + diff_
                        // caffe_axpy的功能是 y = a*x + y
                        // 第一个参数是数据的个数，第二个是上式的a，第三个是x的指针，第四个是y的指针
                        caffe_axpy(net_params[param_id]->count(),
                                   local_decay,
                                   net_params[param_id]->cpu_data(),
                                   net_params[param_id]->mutable_cpu_diff());
                    }
                    else if (regularization_type == "L1") {
                        // L1的梯度为diff_ = diff_ + sign(data_)
                        // temp_ = sign(data_)
                        caffe_cpu_sign(net_params[param_id]->count(),
                                       net_params[param_id]->cpu_data(),
                                       temp_[param_id]->mutable_cpu_data());
                        // 将temp_加到diff_中 diff_ = weight_decay*temp_ + diff_
                        caffe_axpy(net_params[param_id]->count(),
                                   local_decay,
                                   temp_[param_id]->cpu_data(),
                                   net_params[param_id]->mutable_cpu_diff());
                    } else {
                        LOG(FATAL) << "Unknown regularization type: " << regularization_type;
                    }
                }
                break;
            }
            case Caffe::GPU: {
#ifndef CPU_ONLY
//                if (local_decay) {
//      if (regularization_type == "L2") {
//        // add weight decay
//        caffe_gpu_axpy(net_params[param_id]->count(),
//            local_decay,
//            net_params[param_id]->gpu_data(),
//            net_params[param_id]->mutable_gpu_diff());
//      } else if (regularization_type == "L1") {
//        caffe_gpu_sign(net_params[param_id]->count(),
//            net_params[param_id]->gpu_data(),
//            temp_[param_id]->mutable_gpu_data());
//        caffe_gpu_axpy(net_params[param_id]->count(),
//            local_decay,
//            temp_[param_id]->gpu_data(),
//            net_params[param_id]->mutable_gpu_diff());
//      } else {
//        LOG(FATAL) << "Unknown regularization type: " << regularization_type;
//      }
//    }
//#else
                NO_GPU;
#endif
                break;
            }
            default:
                LOG(FATAL) << "Unknown caffe mode: " << Caffe::mode();
        }
    }


    template <typename Dtype>
    void SGDSolver<Dtype>::ComputeUpdateValue(int param_id, Dtype rate) {

        // 获取所有可以更新的参数的vector,需要学习的参数
        const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();

        // 获取所有参数对应的learning_rate的vector
        const vector<float>& net_params_lr = this->net_->params_lr();

        // 获取momentum数值，在solver.prototxt中定义
        Dtype momentum = this->param_.momentum();

        // 实际的learning_rate为全局的learning_rate乘以每个参数对应的learning_rate(倍数)
        Dtype local_rate = rate * net_params_lr[param_id];

        // Compute the update to history, then copy it to the parameter diff.
        switch (Caffe::mode()) {
            case Caffe::CPU: {
                // 关于SGD的公式参考caffe官网tutorial的Solver部分
                // history_存储了上一次的梯度，下面这个函数：
                // history_ = learning_rate*diff_ + momentum*history
                caffe_cpu_axpby(net_params[param_id]->count(), local_rate,
                                net_params[param_id]->cpu_diff(), momentum,
                                history_[param_id]->mutable_cpu_data());

                // 把当前的梯度拷贝给参数Blob的diff_
                caffe_copy(net_params[param_id]->count(),
                           history_[param_id]->cpu_data(),
                           net_params[param_id]->mutable_cpu_diff());
                break;
            }
            case Caffe::GPU: {
#ifndef CPU_ONLY
//                sgd_update_gpu(net_params[param_id]->count(),
//        net_params[param_id]->mutable_gpu_diff(),
//        history_[param_id]->mutable_gpu_data(),
//        momentum, local_rate);
#else
                NO_GPU;
#endif
                break;
            }
            default:
                LOG(FATAL) << "Unknown caffe mode: " << Caffe::mode();
        }
    }

    template <typename Dtype>
    void SGDSolver<Dtype>::SnapshotSolverState(const string& model_filename) {
        switch (this->param_.snapshot_format()) {
            case caffe::SolverParameter_SnapshotFormat_BINARYPROTO:
                SnapshotSolverStateToBinaryProto(model_filename);
                break;
            case caffe::SolverParameter_SnapshotFormat_HDF5:
//                SnapshotSolverStateToHDF5(model_filename);
//                break;
            default:
                LOG(FATAL) << "Unsupported snapshot format.";
        }
    }

    template <typename Dtype>
    void SGDSolver<Dtype>::SnapshotSolverStateToBinaryProto(
            const string& model_filename) {
        SolverState state;
        state.set_iter(this->iter_);
        state.set_learned_net(model_filename);
        state.set_current_step(this->current_step_);
        state.clear_history();
        for (int i = 0; i < history_.size(); ++i) {
            // Add history
            BlobProto* history_blob = state.add_history();
            history_[i]->ToProto(history_blob);
        }
        string snapshot_filename = Solver<Dtype>::SnapshotFilename(".solverstate");
        LOG(INFO)
                << "Snapshotting solver state to binary proto file " << snapshot_filename;
        WriteProtoToBinaryFile(state, snapshot_filename.c_str());
    }


    template <typename Dtype>
    void SGDSolver<Dtype>::SnapshotSolverStateToHDF5(
            const string& model_filename) {
//        string snapshot_filename =
//                Solver<Dtype>::SnapshotFilename(".solverstate.h5");
//        LOG(INFO) << "Snapshotting solver state to HDF5 file " << snapshot_filename;
//        hid_t file_hid = H5Fcreate(snapshot_filename.c_str(), H5F_ACC_TRUNC,
//                                   H5P_DEFAULT, H5P_DEFAULT);
//        CHECK_GE(file_hid, 0)
//            << "Couldn't open " << snapshot_filename << " to save solver state.";
//        hdf5_save_int(file_hid, "iter", this->iter_);
//        hdf5_save_string(file_hid, "learned_net", model_filename);
//        hdf5_save_int(file_hid, "current_step", this->current_step_);
//        hid_t history_hid = H5Gcreate2(file_hid, "history", H5P_DEFAULT, H5P_DEFAULT,
//                                       H5P_DEFAULT);
//        CHECK_GE(history_hid, 0)
//            << "Error saving solver state to " << snapshot_filename << ".";
//        for (int i = 0; i < history_.size(); ++i) {
//            ostringstream oss;
//            oss << i;
//            hdf5_save_nd_dataset<Dtype>(history_hid, oss.str(), *history_[i]);
//        }
//        H5Gclose(history_hid);
//        H5Fclose(file_hid);
        LOG(FATAL) << "Unsupported SnapshotSolverStateToHDF5 format.";
    }

    template <typename Dtype>
    void SGDSolver<Dtype>::RestoreSolverStateFromBinaryProto(
            const string& state_file) {
        SolverState state;
        ReadProtoFromBinaryFile(state_file, &state);
        this->iter_ = state.iter();
        if (state.has_learned_net()) {
            NetParameter net_param;
            ReadNetParamsFromBinaryFileOrDie(state.learned_net().c_str(), &net_param);
            this->net_->CopyTrainedLayersFrom(net_param);
        }
        this->current_step_ = state.current_step();
        CHECK_EQ(state.history_size(), history_.size())
            << "Incorrect length of history blobs.";
        LOG(INFO) << "SGDSolver: restoring history";
        for (int i = 0; i < history_.size(); ++i) {
            history_[i]->FromProto(state.history(i));
        }
    }

    template <typename Dtype>
    void SGDSolver<Dtype>::RestoreSolverStateFromHDF5(const string& state_file) {
//        hid_t file_hid = H5Fopen(state_file.c_str(), H5F_ACC_RDONLY, H5P_DEFAULT);
//        CHECK_GE(file_hid, 0) << "Couldn't open solver state file " << state_file;
//        this->iter_ = hdf5_load_int(file_hid, "iter");
//        if (H5LTfind_dataset(file_hid, "learned_net")) {
//            string learned_net = hdf5_load_string(file_hid, "learned_net");
//            this->net_->CopyTrainedLayersFrom(learned_net);
//        }
//        this->current_step_ = hdf5_load_int(file_hid, "current_step");
//        hid_t history_hid = H5Gopen2(file_hid, "history", H5P_DEFAULT);
//        CHECK_GE(history_hid, 0) << "Error reading history from " << state_file;
//        int state_history_size = hdf5_get_num_links(history_hid);
//        CHECK_EQ(state_history_size, history_.size())
//            << "Incorrect length of history blobs.";
//        for (int i = 0; i < history_.size(); ++i) {
//            ostringstream oss;
//            oss << i;
//            hdf5_load_nd_dataset<Dtype>(history_hid, oss.str().c_str(), 0,
//                                        kMaxBlobAxes, history_[i].get());
//        }
//        H5Gclose(history_hid);
//        H5Fclose(file_hid);
        LOG(FATAL) << "Unsupported RestoreSolverStateFromHDF5 format.";
    }

///模板显示实例化
    INSTANTIATE_CLASS(SGDSolver);

    //全局变量初始化
    REGISTER_SOLVER_CLASS(SGD);

}